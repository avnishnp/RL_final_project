{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout-V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the graphs generated get stored to an experiment tracking platform i.e. Weights & Biases (wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    ")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.login()\n",
    "\n",
    "class BreakoutPPONet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for the PPO algorithm in the Breakout environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: Tuple[int, int, int], num_actions: int):\n",
    "        \"\"\"\n",
    "        Initializes the Breakout PPO network.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): Shape of the input (channels, height, width).\n",
    "            num_actions (int): Number of actions in the action space.\n",
    "        \"\"\"\n",
    "        super(BreakoutPPONet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        def conv2d_size_out(size: int, kernel_size: int, stride: int) -> int:\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[1], 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[2], 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "        self.fc = nn.Linear(linear_input_size, 512)\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing the state.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Actor logits and critic value.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get action, log probability, entropy, and value for a given state.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): Current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Action, log probability, entropy, and value.\n",
    "        \"\"\"\n",
    "        logits, value = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the PPO algorithm in the Breakout environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, params: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the PPO trainer.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment for training.\n",
    "            params (Dict[str, Any]): Dictionary of training parameters.\n",
    "        \"\"\"\n",
    "        wandb.init(project=\"breakout-ppo\", config=params)\n",
    "        self.env = env\n",
    "        self.params = params\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.agent = BreakoutPPONet((4, 84, 84), self.num_actions).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=params[\"learning_rate\"])\n",
    "        self.gamma = params[\"gamma\"]\n",
    "        self.lam = params[\"gae_lambda\"]\n",
    "        self.clip_epsilon = params[\"clip_epsilon\"]\n",
    "        self.entropy_coef = params[\"entropy_coef\"]\n",
    "        self.value_coef = params[\"value_coef\"]\n",
    "        self.max_grad_norm = params[\"max_grad_norm\"]\n",
    "        self.frames = []\n",
    "        self.episode_returns = []\n",
    "        self.episode_losses = []\n",
    "        self.episode_lengths = []\n",
    "        self.moving_avg_window = 15\n",
    "\n",
    "    def preprocess_obs(self, obs: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Preprocesses the observation for the neural network.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): Raw observation from the environment.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Preprocessed observation.\n",
    "        \"\"\"\n",
    "        return torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "    \n",
    "    def calculate_moving_average(self, data: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the moving average of the provided data.\n",
    "\n",
    "        Args:\n",
    "            data (List[float]): Data series to calculate the moving average.\n",
    "\n",
    "        Returns:\n",
    "            float: Moving average of the data.\n",
    "        \"\"\"\n",
    "        return pd.Series(data).rolling(window=self.moving_avg_window).mean().iloc[-1]\n",
    "\n",
    "    def collect_rollout(self) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[float], List[torch.Tensor], List[torch.Tensor], List[bool], List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Collects a rollout from the environment.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[torch.Tensor], List[torch.Tensor], List[float], List[torch.Tensor], List[torch.Tensor], List[bool], List[torch.Tensor]]:\n",
    "                States, actions, rewards, values, log probabilities, done flags, and entropies from the rollout.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, values, log_probs, dones, entropies = [], [], [], [], [], [], []\n",
    "        max_steps = 2048\n",
    "        step = 0\n",
    "        obs, _ = self.env.reset()\n",
    "        obs = self.preprocess_obs(obs)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while step < max_steps:\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, entropy, value = self.agent.get_action_and_value(obs)\n",
    "            next_obs, reward, done, truncated, _ = self.env.step(action.cpu().numpy()[0])\n",
    "            next_obs = self.preprocess_obs(next_obs)\n",
    "            states.append(obs.detach())\n",
    "            actions.append(action.detach())\n",
    "            rewards.append(reward)\n",
    "            values.append(value.detach())\n",
    "            log_probs.append(log_prob.detach())\n",
    "            entropies.append(entropy.detach())\n",
    "            dones.append(done or truncated)\n",
    "            episode_reward += reward\n",
    "            obs = next_obs\n",
    "            step += 1\n",
    "            if done or truncated:\n",
    "                obs, _ = self.env.reset()\n",
    "                obs = self.preprocess_obs(obs)\n",
    "                self.episode_returns.append(episode_reward)\n",
    "                self.episode_lengths.append(len(rewards))\n",
    "                episode_reward = 0\n",
    "        self.episode_returns.append(episode_reward)\n",
    "        self.episode_lengths.append(len(rewards))\n",
    "        return states, actions, rewards, values, log_probs, dones, entropies\n",
    "\n",
    "    def compute_gae(self, rewards: List[float], values: List[torch.Tensor], dones: List[bool]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes Generalized Advantage Estimation (GAE).\n",
    "\n",
    "        Args:\n",
    "            rewards (List[float]): List of rewards from the rollout.\n",
    "            values (List[torch.Tensor]): List of value estimates.\n",
    "            dones (List[bool]): List of done flags.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Computed advantages and returns.\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        advantages, returns = [], []\n",
    "        last_gae = 0\n",
    "        next_value = 0 if dones[-1] else values[-1].item()\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t].item()\n",
    "            last_gae = delta + self.gamma * self.lam * (1 - dones[t]) * last_gae\n",
    "            advantages.insert(0, last_gae)\n",
    "            returns.insert(0, last_gae + values[t].item())\n",
    "            next_value = values[t].item()\n",
    "        return (torch.tensor(advantages, dtype=torch.float32).detach().to(device),\n",
    "                torch.tensor(returns, dtype=torch.float32).detach().to(device))\n",
    "\n",
    "    def update_policy(self, states: List[torch.Tensor], actions: List[torch.Tensor], returns: torch.Tensor, advantages: torch.Tensor, old_log_probs: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Updates the policy network using PPO.\n",
    "\n",
    "        Args:\n",
    "            states (List[torch.Tensor]): Collected states from the rollout.\n",
    "            actions (List[torch.Tensor]): Actions taken.\n",
    "            returns (torch.Tensor): Returns computed from GAE.\n",
    "            advantages (torch.Tensor): Advantages computed from GAE.\n",
    "            old_log_probs (List[torch.Tensor]): Log probabilities of the old actions.\n",
    "        \"\"\"\n",
    "        states = torch.cat(states).to(device)\n",
    "        actions = torch.cat(actions).to(device)\n",
    "        old_log_probs = torch.cat(old_log_probs).to(device)\n",
    "        batch_size = 64\n",
    "        indices = np.arange(states.shape[0])\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(self.params[\"update_epochs\"]):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                \n",
    "                logits, values = self.agent(batch_states)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = F.mse_loss(values.squeeze(-1), batch_returns)\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.agent.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                self.episode_losses.append(loss.item())\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Saves the trained model to disk and logs it to W&B.\n",
    "        \"\"\"\n",
    "        artifact = wandb.Artifact('ppo_model', type='model')\n",
    "        path = 'ppo_breakout_model.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': self.agent.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'episode_returns': self.episode_returns,\n",
    "            'episode_losses': self.episode_losses,\n",
    "            'episode_lengths': self.episode_lengths,\n",
    "            'params': self.params\n",
    "        }, path)\n",
    "        artifact.add_file(path)\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop for the PPO algorithm.\n",
    "        \"\"\"\n",
    "        for episode in range(self.params[\"num_episodes\"]):\n",
    "            states, actions, rewards, values, log_probs, dones, entropies = self.collect_rollout()\n",
    "            advantages, returns = self.compute_gae(rewards, values, dones)\n",
    "            self.update_policy(states, actions, returns, advantages, log_probs)\n",
    "            episode_reward = self.episode_returns[-1]\n",
    "            reward_moving_avg = self.calculate_moving_average(self.episode_returns)\n",
    "            loss_moving_avg = self.calculate_moving_average(self.episode_losses)\n",
    "            entropy_avg = np.mean([entropy.item() for entropy in entropies])\n",
    "            wandb.log({\n",
    "                \"Reward\": {\"Raw\": episode_reward, \"Moving Avg\": reward_moving_avg},\n",
    "                \"Loss\": {\"Raw\": self.episode_losses[-1], \"Moving Avg\": loss_moving_avg},\n",
    "                \"Entropy\": entropy_avg,\n",
    "            })\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}, Moving Avg Reward = {reward_moving_avg:.2f}, Entropy = {entropy_avg:.4f}\")\n",
    "        self.save_model()\n",
    "        wandb.finish()\n",
    "\n",
    "def make_env() -> gym.Env:\n",
    "    \"\"\"\n",
    "    Creates and wraps the Breakout environment.\n",
    "\n",
    "    Returns:\n",
    "        gym.Env: The wrapped Breakout environment.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\")\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "    env = gym.wrappers.GrayscaleObservation(env)\n",
    "    env = gym.wrappers.FrameStackObservation(env, 4)\n",
    "    return env\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define default configuration\n",
    "    config_defaults = {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"clip_epsilon\": 0.2,\n",
    "        \"entropy_coef\": 0.01,\n",
    "        \"value_coef\": 0.500,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"num_episodes\": 4000,\n",
    "        \"update_epochs\": 10,\n",
    "    }\n",
    "\n",
    "    # Initialize a W&B run with the default configuration\n",
    "    wandb.init(project=\"breakout-ppo\", config=config_defaults)\n",
    "\n",
    "    # Get the configuration from W&B\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize the environment and the trainer\n",
    "    env = make_env()\n",
    "    trainer = PPOTrainer(env, config)\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Clean up\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    ")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.login()\n",
    "\n",
    "class BreakoutPPONet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for PPO to handle Breakout game.\n",
    "\n",
    "    Attributes:\n",
    "        conv1, conv2, conv3: Convolutional layers.\n",
    "        fc: Fully connected layer for feature extraction.\n",
    "        actor: Fully connected layer for policy output.\n",
    "        critic: Fully connected layer for value estimation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: Tuple[int, int, int], num_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize the Breakout PPO network.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): The shape of the input (channels, height, width).\n",
    "            num_actions (int): The number of possible actions.\n",
    "        \"\"\"\n",
    "        super(BreakoutPPONet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        def conv2d_size_out(size: int, kernel_size: int, stride: int) -> int:\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[1], 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[2], 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "\n",
    "        self.fc = nn.Linear(linear_input_size, 512)\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The logits for actions and value estimates.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get the action, log probability, entropy, and value for a given state.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The input state.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                The action, log probability, entropy, and value.\n",
    "        \"\"\"\n",
    "        logits, value = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for PPO on the Breakout environment.\n",
    "\n",
    "    Attributes:\n",
    "        env: The game environment.\n",
    "        params: Training parameters.\n",
    "        agent: The PPO network.\n",
    "        optimizer: Optimizer for training the network.\n",
    "        gamma, lam: Discount factors for rewards and GAE.\n",
    "        clip_epsilon: Clipping epsilon for PPO.\n",
    "        entropy_coef, value_coef: Coefficients for entropy and value loss.\n",
    "        max_grad_norm: Maximum gradient norm for clipping.\n",
    "        frames, episode_returns, episode_losses, episode_lengths: Tracking data.\n",
    "        moving_avg_window: Window size for moving average calculation.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, params: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the PPO trainer.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The game environment.\n",
    "            params (Dict[str, Any]): Dictionary of training parameters.\n",
    "        \"\"\"\n",
    "        wandb.init(project=\"breakout-ppo\", config=params)\n",
    "        self.env = env\n",
    "        self.params = params\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.agent = BreakoutPPONet((4, 84, 84), self.num_actions).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=params[\"learning_rate\"])\n",
    "        self.gamma = params[\"gamma\"]\n",
    "        self.lam = params[\"gae_lambda\"]\n",
    "        self.clip_epsilon = params[\"clip_epsilon\"]\n",
    "        self.entropy_coef = params[\"entropy_coef\"]\n",
    "        self.value_coef = params[\"value_coef\"]\n",
    "        self.max_grad_norm = params[\"max_grad_norm\"]\n",
    "        self.frames = []\n",
    "        self.episode_returns = []\n",
    "        self.episode_losses = []\n",
    "        self.episode_lengths = []\n",
    "        self.moving_avg_window = 15\n",
    "\n",
    "    def preprocess_obs(self, obs: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Preprocess the observation for the model.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): The raw observation.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The preprocessed observation.\n",
    "        \"\"\"\n",
    "        return torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "\n",
    "    def calculate_moving_average(self, data: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the moving average of a data series.\n",
    "\n",
    "        Args:\n",
    "            data (List[float]): The data series.\n",
    "\n",
    "        Returns:\n",
    "            float: The moving average.\n",
    "        \"\"\"\n",
    "        return pd.Series(data).rolling(window=self.moving_avg_window).mean().iloc[-1]\n",
    "\n",
    "    def collect_rollout(self) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[float], List[torch.Tensor], List[torch.Tensor], List[bool], List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Collect a rollout from the environment.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing states, actions, rewards, values, log_probs, dones, and entropies.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, values, log_probs, dones, entropies = [], [], [], [], [], [], []\n",
    "        max_steps = 2048\n",
    "        step = 0\n",
    "        obs, _ = self.env.reset()\n",
    "        obs = self.preprocess_obs(obs)\n",
    "        episode_reward = 0\n",
    "\n",
    "        while step < max_steps:\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, entropy, value = self.agent.get_action_and_value(obs)\n",
    "            next_obs, reward, done, truncated, _ = self.env.step(action.cpu().numpy()[0])\n",
    "            next_obs = self.preprocess_obs(next_obs)\n",
    "            states.append(obs.detach())\n",
    "            actions.append(action.detach())\n",
    "            rewards.append(reward)\n",
    "            values.append(value.detach())\n",
    "            log_probs.append(log_prob.detach())\n",
    "            entropies.append(entropy.detach())\n",
    "            dones.append(done or truncated)\n",
    "            episode_reward += reward\n",
    "            obs = next_obs\n",
    "            step += 1\n",
    "            if done or truncated:\n",
    "                obs, _ = self.env.reset()\n",
    "                obs = self.preprocess_obs(obs)\n",
    "                self.episode_returns.append(episode_reward)\n",
    "                self.episode_lengths.append(len(rewards))\n",
    "                episode_reward = 0\n",
    "        self.episode_returns.append(episode_reward)\n",
    "        self.episode_lengths.append(len(rewards))\n",
    "        return states, actions, rewards, values, log_probs, dones, entropies\n",
    "\n",
    "    def compute_gae(self, rewards: List[float], values: List[torch.Tensor], dones: List[bool]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the Generalized Advantage Estimate (GAE).\n",
    "\n",
    "        Args:\n",
    "            rewards (List[float]): List of rewards.\n",
    "            values (List[torch.Tensor]): List of value estimates.\n",
    "            dones (List[bool]): List of done flags.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Advantages and returns.\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        advantages, returns = [], []\n",
    "        last_gae = 0\n",
    "        next_value = 0 if dones[-1] else values[-1].item()\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t].item()\n",
    "            last_gae = delta + self.gamma * self.lam * (1 - dones[t]) * last_gae\n",
    "            advantages.insert(0, last_gae)\n",
    "            returns.insert(0, last_gae + values[t].item())\n",
    "            next_value = values[t].item()\n",
    "        return (torch.tensor(advantages, dtype=torch.float32).detach().to(device),\n",
    "                torch.tensor(returns, dtype=torch.float32).detach().to(device))\n",
    "\n",
    "    def update_policy(self, states: List[torch.Tensor], actions: List[torch.Tensor], returns: torch.Tensor, advantages: torch.Tensor, old_log_probs: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Update the policy using PPO.\n",
    "\n",
    "        Args:\n",
    "            states (List[torch.Tensor]): Collected states.\n",
    "            actions (List[torch.Tensor]): Actions taken.\n",
    "            returns (torch.Tensor): Discounted returns.\n",
    "            advantages (torch.Tensor): Computed advantages.\n",
    "            old_log_probs (List[torch.Tensor]): Old log probabilities of actions.\n",
    "        \"\"\"\n",
    "        states = torch.cat(states).to(device)\n",
    "        actions = torch.cat(actions).to(device)\n",
    "        old_log_probs = torch.cat(old_log_probs).to(device)\n",
    "        batch_size = 64\n",
    "        indices = np.arange(states.shape[0])\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(self.params[\"update_epochs\"]):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "\n",
    "                logits, values = self.agent(batch_states)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = F.mse_loss(values.squeeze(-1), batch_returns)\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.agent.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                self.episode_losses.append(loss.item())\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Save the trained model to disk and log it to W&B.\n",
    "        \"\"\"\n",
    "        artifact = wandb.Artifact('ppo_model', type='model')\n",
    "        path = 'ppo_breakout_model.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': self.agent.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'episode_returns': self.episode_returns,\n",
    "            'episode_losses': self.episode_losses,\n",
    "            'episode_lengths': self.episode_lengths,\n",
    "            'params': self.params\n",
    "        }, path)\n",
    "        artifact.add_file(path)\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop for PPO.\n",
    "        \"\"\"\n",
    "        for episode in range(self.params[\"num_episodes\"]):\n",
    "            states, actions, rewards, values, log_probs, dones, entropies = self.collect_rollout()\n",
    "            advantages, returns = self.compute_gae(rewards, values, dones)\n",
    "            self.update_policy(states, actions, returns, advantages, log_probs)\n",
    "            episode_reward = self.episode_returns[-1]\n",
    "            reward_moving_avg = self.calculate_moving_average(self.episode_returns)\n",
    "            loss_moving_avg = self.calculate_moving_average(self.episode_losses)\n",
    "            entropy_avg = np.mean([entropy.item() for entropy in entropies])\n",
    "            wandb.log({\n",
    "                \"Reward\": {\"Raw\": episode_reward, \"Moving Avg\": reward_moving_avg},\n",
    "                \"Loss\": {\"Raw\": self.episode_losses[-1], \"Moving Avg\": loss_moving_avg},\n",
    "                \"Entropy\": entropy_avg,\n",
    "            })\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}, Moving Avg Reward = {reward_moving_avg:.2f}, Entropy = {entropy_avg:.4f}\")\n",
    "        self.save_model()\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "def make_env() -> gym.Env:\n",
    "    \"\"\"\n",
    "    Create and wrap the Breakout environment.\n",
    "\n",
    "    Returns:\n",
    "        gym.Env: The wrapped Breakout environment.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\")\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "    env = gym.wrappers.GrayscaleObservation(env)\n",
    "    env = gym.wrappers.FrameStackObservation(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define default configuration\n",
    "    config_defaults = {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"clip_epsilon\": 0.2,\n",
    "        \"entropy_coef\": 0.01,\n",
    "        \"value_coef\": 0.500,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"num_episodes\": 4000,\n",
    "        \"update_epochs\": 10,\n",
    "    }\n",
    "\n",
    "    # Initialize a W&B run with the default configuration\n",
    "    wandb.init(project=\"pong-ppo\", config=config_defaults)\n",
    "\n",
    "    # Get the configuration from W&B\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize the environment and the trainer\n",
    "    env = make_env()\n",
    "    trainer = PPOTrainer(env, config)\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Clean up\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unimatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
